#!/usr/bin/env python

import argparse
from copy import deepcopy
from datetime import datetime
import json
import os
import requests
import sys
import time
from csv import writer

# these moved around from python 2 to 3
try:
    from urllib2 import urlparse
    urlparse = urlparse.urlparse
except ImportError:
    from urllib.parse import urlparse

# background: we want to run a number of tests, each of which consists
# of sending an increasing number of terms (either quoted or unquoted)
# against a solr instance (assumed to be running on a particular host)

# rationale: edismax and dismax parsers create boolean clauses 
# via their operation, and you may run into the maxBooleanClauses
# restriction in Solr (especially since Solr 8+ enforces a default of
# 1024 for this value); the results generated by this script may help
# you dial in a sensible value to set this to, or locate problems 
# with various queries.

# the Solr results are stored in the `json` subdirectory (which will 
# be created if it does not exist) using the pattern 
# [hostname]-test-[testno]-terms-[num terms]-[quoted/unquoted].json
# these are kept so analysis can be done on the results later, if 
# desired.

# in addition, CSV files with the pattern
# [hostname]-test-[testno].csv 
# are created in the current directory.  These can be fed directly to the
# analyze.py script in this directory 

# requirements: pip install --user requests # or use your OS package manager

parser = argparse.ArgumentParser(description="Run a series of queries against Solr and collect some statistics")

# edit choices and default as you see fit
# sample usage: ./qp.py -n 1 -c blacklight --query-file test_queries.txt mysolrhost
# runs test number 1 against the 'blacklight' collection using queries found in queries.txt against http://mysolrhost:8983/solr

parser.add_argument('-n', type=int, help='number of run in the test; used to generate filenames', default=1)
parser.add_argument('-c', '--collection', type=str,help="Collection to run the queries against", default='_default')
parser.add_argument('-t', '--test-name', type=str, help="Test name (for stats)", default='default')
parser.add_argument('-q', '--quote', type=bool, help="whether to wrap terms in double quotes", default=False)
parser.add_argument('--query-file', type=str, help="A file to read queries from, one per line")
parser.add_argument('--failsafe-threshold', type=int, help="Maximum boolean clauses to allow", default=40000)
parser.add_argument('--default-query', type=str, help="A JSON file to read default query params from (rows, defType, etc.)")
parser.add_argument('--query-dir', type=str, help="A directory to read queries and query defaults from (queries.txt,params.json)")
parser.add_argument('-u', '--url', type=str, help="Full URL to Solr collection; use for more control than provided by setting host")
parser.add_argument('host', nargs='?', type=str, help='Solr Hostname -- will derive URL as http://{host}:8983/solr/{collection}/select', default='localhost')

args = parser.parse_args()

class Configuration:
    def __init__(self, args):
        self.args = args
        self.solr_url = self.process_url()
        self.failsafe_threshold = args.failsafe_threshold
        self.test_name = args.test_name or 'default'
        self.query_prefix = ''

        if args.query_dir:
            qfile = os.path.join(args.query_dir,'queries.txt')
            pfile = os.path.join(args.query_dir,'params.json')
            if os.path.exists(qfile) and args.query_file is None:
                args.query_file = qfile
            if os.path.exists(pfile) and args.default_query is None:
                args.default_query = pfile

        self.quoted = args.quote
        self._default_query = { 'debug': 'true',
                                'rows': 0 }
        if args.default_query is not None:
            with open(args.default_query) as f:
                self._default_query = json.load(f)
                self._default_query['debug'] = 'true'
                self.query_prefix = self._default_query.pop(':_prefix', '')

        self.queries = self.load_queries()
        
    @property
    def collection(self):
        return self.args.collection

    @property
    def base_query(self):
       return deepcopy(self._default_query) 

    def load_queries(self):
        if self.args.query_file is None:
            # a silly sentence we will extend through as we run through the
            # test cases.  Make your own, although this sort of thing might
            # be useful for getting a sense of how fast your BQs rise as
            # the number of terms rise
            terms = 'now is the time for all good otters to come with a pelagic spongy clamshell made of abalone and cuteness'.split()
            return [self.query_prefix + " ".join(terms[:i]) for i in range(4, len(terms)+1)]

        with open(self.args.query_file) as f:
            return [self.query_prefix + x.strip() for x in f if x.strip() and not x.startswith('#')]

    def process_url(self):
       if self.args.url is not None:
           self.host = urlparse(self.args.url).netloc
           return self.args.url
       return "http://%s:8983/solr/%s/select" %( self.args.host, self.args.collection )

    def __repr__(self):
        return "Configuration(solr_url=%s,n=%d,quoted=%s, queries=%s, default_query: %s, prefix=%s)" % ( self.solr_url, self.args.n, self.args.quote, self.args.query_file and self.args.query_file or '(defaults)', self._default_query, self.query_prefix)

def collect_data(config, csvw):
    for i, query in enumerate(config.queries):
        # yeah we could ask Solr how long it took to run the query =)
        st = time.time()
        resp = requests.get(config.solr_url,
                params= dict(config.base_query, q=query)
                          )
        elapsed = round((time.time() -st)*1000)
        r = resp.json()
        solr_qtime = r['responseHeader']['QTime']
        # stash the query response
        if not os.path.isdir('json'):
            os.mkdir('json')
        query_result_path = "%s-test-%d-terms-%d-%s.json" % ( config.args.host, config.args.n, len(query.split()), config.test_name)
        with open(os.path.join('json', query_result_path), 'w') as f:
            json.dump(r, f)

        # remove the prefix from the query to get a better count of the 
        # number of terms
        user_query = query[len(config.query_prefix):]
        num_terms = config.quoted and 1 or len(user_query.split())
        
        
        # count the number of boolean clauses generated, assumed to be 
        # OR / | 
        pq = r['debug']['parsedquery']
        result_count = r['response']['numFound']
        numclauses = len(pq.split(' | '))
        csvw.writerow((
            config.args.host,
            config.args.n,
            config.test_name,
            num_terms,
            result_count,
            elapsed,
            solr_qtime,
            numclauses,
            query)
        )

        if numclauses > config.failsafe_threshold:
            print("Solr has suffered enough, stopping it there")
            break


config = Configuration(args)
if not os.path.isdir('results'):
    os.mkdir('results')
dest = os.path.join('results', "%s-test-%d-%s.csv" % ( args.host, args.n, config.test_name))

with open(dest, 'w') as f:
    csvw = writer(f)
    csvw.writerow(('host', 'test_number', 'test_type', 'querylen','result_count','time', 'solr_qtime', 'num_clauses','query'))
    collect_data(config, csvw)
